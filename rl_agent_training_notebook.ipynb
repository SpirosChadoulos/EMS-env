{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from EMS_Gym_Env import EMSGymEnv\n",
    "from device_classes import Intermittent, Uninterruptible\n",
    "\n",
    "import ray\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "from ray import tune\n",
    "\n",
    "from ray.rllib.algorithms.algorithm_config import AlgorithmConfig\n",
    "from ray.rllib.algorithms.callbacks import DefaultCallbacks\n",
    "from ray.air import RunConfig, CheckpointConfig\n",
    "\n",
    "from typing import Dict, Optional, Union\n",
    "\n",
    "from ray.rllib.env.base_env import BaseEnv\n",
    "from ray.rllib.evaluation.episode import Episode\n",
    "from ray.rllib.evaluation.episode_v2 import EpisodeV2\n",
    "from ray.rllib.policy import Policy\n",
    "from ray.rllib.utils.typing import PolicyID\n",
    "from ray.rllib.evaluation import RolloutWorker\n",
    "from ray.rllib.utils.deprecation import deprecation_warning\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Custom Environment based on information found at : https://www.daftlogic.com/information-appliance-power-consumption.htm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User that accepts most changes - Receptive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment successfully initialized\n"
     ]
    }
   ],
   "source": [
    "# Assigning data file\n",
    "#data_file = \"prices_for_one_day_inference.csv\"\n",
    "data_file = \"nyiso_hourly_prices.csv\"\n",
    "\n",
    "# Creating the intermittent devices\n",
    "intermittent_user_probabilities = np.array([0.90, 0.90, 0.90, 0.90])\n",
    "\n",
    "intermittent_device_penalty = 100\n",
    "\n",
    "intermittent_device_1 = Intermittent(name = \"Small AC\", device_power_consumption = 1, \n",
    "                                        user_probabilities = intermittent_user_probabilities, device_standard_penalty = intermittent_device_penalty)\n",
    "\n",
    "intermittent_device_2 = Intermittent(name = \"Big AC\", device_power_consumption = 2.5, \n",
    "                                    user_probabilities = intermittent_user_probabilities, device_standard_penalty = intermittent_device_penalty)\n",
    "\n",
    "intermittent_device_3 = Intermittent(name = \"Ceiling Fan 1\", device_power_consumption = 0.07,\n",
    "                                    user_probabilities = intermittent_user_probabilities, device_standard_penalty = intermittent_device_penalty)\n",
    "\n",
    "intermittent_device_4 = Intermittent(name = \"Ceiling Fan 2\", device_power_consumption = 0.07,\n",
    "                                    user_probabilities = intermittent_user_probabilities, device_standard_penalty = intermittent_device_penalty)\n",
    "\n",
    "intermitent_device_5 = Intermittent(name = \"Boiler\", device_power_consumption = 3,\n",
    "                                    user_probabilities = intermittent_user_probabilities, device_standard_penalty = intermittent_device_penalty)\n",
    "\n",
    "intermittent_device_6 = Intermittent(name = \"Dehumidifier\", device_power_consumption = 0.07,\n",
    "                                    user_probabilities = intermittent_user_probabilities, device_standard_penalty = intermittent_device_penalty)\n",
    "\n",
    "array_of_intermittent_devices = [intermittent_device_1, intermittent_device_2, intermittent_device_3, intermittent_device_4, intermitent_device_5, intermittent_device_6]\n",
    "\n",
    "# Creating the uninterruptible devices\n",
    "\n",
    "uninterruptible_user_probabilities = np.array([0.90, 0.90, 0.90, 0.90])\n",
    "\n",
    "uninterruptible_device_standard_penalty = 100\n",
    "\n",
    "uninterruptible_device_override_penalty = 1000\n",
    "\n",
    "uninterruptible_device_1 = Uninterruptible(name = \"Dishwasher\", device_power_consumption = 1.3,\n",
    "                                            user_probabilities = uninterruptible_user_probabilities, device_standard_penalty = uninterruptible_device_standard_penalty, \n",
    "                                            device_on_duration = 2.5, device_override_penalty = uninterruptible_device_override_penalty)\n",
    "\n",
    "uninterruptible_device_2 = Uninterruptible(name = \"Washing Machine\", device_power_consumption = 0.5,\n",
    "                                            user_probabilities = uninterruptible_user_probabilities, device_standard_penalty = uninterruptible_device_standard_penalty, \n",
    "                                            device_on_duration = 1, device_override_penalty = uninterruptible_device_override_penalty)\n",
    "\n",
    "uninterruptible_device_3 = Uninterruptible(name = \"Clothes Dryer\", device_power_consumption = 2.4,\n",
    "                                            user_probabilities = uninterruptible_user_probabilities, device_standard_penalty = uninterruptible_device_standard_penalty, \n",
    "                                            device_on_duration = 0.5, device_override_penalty = uninterruptible_device_override_penalty)\n",
    "\n",
    "array_of_uninterruptible_devices = [uninterruptible_device_1, uninterruptible_device_2, uninterruptible_device_3]\n",
    "\n",
    "# Creating the environment\n",
    "receptive_train_env = EMSGymEnv(data_file = data_file, intermittent_devices = array_of_intermittent_devices,\n",
    "                uninterruptible_devices = array_of_uninterruptible_devices, episode_horizon = 7, time_step_duration = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User that rejects most changes - Resistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment successfully initialized\n"
     ]
    }
   ],
   "source": [
    "# Assigning data file\n",
    "#data_file = \"prices_for_one_day_inference.csv\"\n",
    "data_file = \"nyiso_hourly_prices.csv\"\n",
    "\n",
    "# Creating the intermittent devices\n",
    "intermittent_user_probabilities = np.array([0.10, 0.10, 0.10, 0.10])\n",
    "\n",
    "intermittent_device_penalty = 100\n",
    "\n",
    "intermittent_device_1 = Intermittent(name = \"Small AC\", device_power_consumption = 1, \n",
    "                                        user_probabilities = intermittent_user_probabilities, device_standard_penalty = intermittent_device_penalty)\n",
    "\n",
    "intermittent_device_2 = Intermittent(name = \"Big AC\", device_power_consumption = 2.5, \n",
    "                                    user_probabilities = intermittent_user_probabilities, device_standard_penalty = intermittent_device_penalty)\n",
    "\n",
    "intermittent_device_3 = Intermittent(name = \"Ceiling Fan 1\", device_power_consumption = 0.07,\n",
    "                                    user_probabilities = intermittent_user_probabilities, device_standard_penalty = intermittent_device_penalty)\n",
    "\n",
    "intermittent_device_4 = Intermittent(name = \"Ceiling Fan 2\", device_power_consumption = 0.07,\n",
    "                                    user_probabilities = intermittent_user_probabilities, device_standard_penalty = intermittent_device_penalty)\n",
    "\n",
    "intermitent_device_5 = Intermittent(name = \"Boiler\", device_power_consumption = 3,\n",
    "                                    user_probabilities = intermittent_user_probabilities, device_standard_penalty = intermittent_device_penalty)\n",
    "\n",
    "intermittent_device_6 = Intermittent(name = \"Dehumidifier\", device_power_consumption = 0.07,\n",
    "                                    user_probabilities = intermittent_user_probabilities, device_standard_penalty = intermittent_device_penalty)\n",
    "\n",
    "array_of_intermittent_devices = [intermittent_device_1, intermittent_device_2, intermittent_device_3, intermittent_device_4, intermitent_device_5, intermittent_device_6]\n",
    "\n",
    "# Creating the uninterruptible devices\n",
    "\n",
    "uninterruptible_user_probabilities = np.array([0.10, 0.10, 0.10, 0.10])\n",
    "\n",
    "uninterruptible_device_standard_penalty = 100\n",
    "\n",
    "uninterruptible_device_override_penalty = 1000\n",
    "\n",
    "uninterruptible_device_1 = Uninterruptible(name = \"Dishwasher\", device_power_consumption = 1.3,\n",
    "                                            user_probabilities = uninterruptible_user_probabilities, device_standard_penalty = uninterruptible_device_standard_penalty, \n",
    "                                            device_on_duration = 2.5, device_override_penalty = uninterruptible_device_override_penalty)\n",
    "\n",
    "uninterruptible_device_2 = Uninterruptible(name = \"Washing Machine\", device_power_consumption = 0.5,\n",
    "                                            user_probabilities = uninterruptible_user_probabilities, device_standard_penalty = uninterruptible_device_standard_penalty, \n",
    "                                            device_on_duration = 1, device_override_penalty = uninterruptible_device_override_penalty)\n",
    "\n",
    "uninterruptible_device_3 = Uninterruptible(name = \"Clothes Dryer\", device_power_consumption = 2.4,\n",
    "                                            user_probabilities = uninterruptible_user_probabilities, device_standard_penalty = uninterruptible_device_standard_penalty, \n",
    "                                            device_on_duration = 0.5, device_override_penalty = uninterruptible_device_override_penalty)\n",
    "\n",
    "array_of_uninterruptible_devices = [uninterruptible_device_1, uninterruptible_device_2, uninterruptible_device_3]\n",
    "\n",
    "# Creating the environment\n",
    "resistant_train_env = EMSGymEnv(data_file = data_file, intermittent_devices = array_of_intermittent_devices,\n",
    "                uninterruptible_devices = array_of_uninterruptible_devices, episode_horizon = 7, time_step_duration = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neutral User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.47490802 0.59014286 0.54639879 0.5197317 ]\n",
      " [0.43120373 0.4311989  0.41161672 0.57323523]\n",
      " [0.520223   0.54161452 0.4041169  0.59398197]\n",
      " [0.56648853 0.44246782 0.43636499 0.4366809 ]\n",
      " [0.46084845 0.50495129 0.486389   0.45824583]\n",
      " [0.52237058 0.42789877 0.45842893 0.47327237]]\n",
      "[[0.491214   0.55703519 0.43993476 0.50284689]\n",
      " [0.51848291 0.40929008 0.52150897 0.43410482]\n",
      " [0.41301032 0.58977711 0.59312641 0.56167947]]\n",
      "Environment successfully initialized\n"
     ]
    }
   ],
   "source": [
    "# Assigning data file\n",
    "#data_file = \"prices_for_one_day_inference.csv\"\n",
    "data_file = \"nyiso_hourly_prices.csv\"\n",
    "\n",
    "# Set the seed\n",
    "seed_value = 42\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# Generate a (6,4) array with random numbers between 0.4 and 0.6 for the intermittent user\n",
    "user_prob = np.random.uniform(0.4, 0.6, size=(1, 4))\n",
    "\n",
    "# Creating the intermittent devices\n",
    "intermittent_user_probabilities = user_prob\n",
    "\n",
    "intermittent_device_penalty = 100\n",
    "\n",
    "intermittent_device_1 = Intermittent(name = \"Small AC\", device_power_consumption = 1, \n",
    "                                        user_probabilities = intermittent_user_probabilities, device_standard_penalty = intermittent_device_penalty)\n",
    "\n",
    "intermittent_device_2 = Intermittent(name = \"Big AC\", device_power_consumption = 2.5, \n",
    "                                    user_probabilities = intermittent_user_probabilities, device_standard_penalty = intermittent_device_penalty)\n",
    "\n",
    "intermittent_device_3 = Intermittent(name = \"Ceiling Fan 1\", device_power_consumption = 0.07,\n",
    "                                    user_probabilities = intermittent_user_probabilities, device_standard_penalty = intermittent_device_penalty)\n",
    "\n",
    "intermittent_device_4 = Intermittent(name = \"Ceiling Fan 2\", device_power_consumption = 0.07,\n",
    "                                    user_probabilities = intermittent_user_probabilities, device_standard_penalty = intermittent_device_penalty)\n",
    "\n",
    "intermitent_device_5 = Intermittent(name = \"Boiler\", device_power_consumption = 3,\n",
    "                                    user_probabilities = intermittent_user_probabilities, device_standard_penalty = intermittent_device_penalty)\n",
    "\n",
    "intermittent_device_6 = Intermittent(name = \"Dehumidifier\", device_power_consumption = 0.07,\n",
    "                                    user_probabilities = intermittent_user_probabilities, device_standard_penalty = intermittent_device_penalty)\n",
    "\n",
    "array_of_intermittent_devices = [intermittent_device_1, intermittent_device_2, intermittent_device_3, intermittent_device_4, intermitent_device_5, intermittent_device_6]\n",
    "\n",
    "# Creating the uninterruptible devices\n",
    "\n",
    "uninterruptible_user_probabilities = user_prob\n",
    "\n",
    "uninterruptible_device_standard_penalty = 100\n",
    "\n",
    "uninterruptible_device_override_penalty = 1000\n",
    "\n",
    "uninterruptible_device_1 = Uninterruptible(name = \"Dishwasher\", device_power_consumption = 1.3,\n",
    "                                            user_probabilities = uninterruptible_user_probabilities, device_standard_penalty = uninterruptible_device_standard_penalty, \n",
    "                                            device_on_duration = 2.5, device_override_penalty = uninterruptible_device_override_penalty)\n",
    "\n",
    "uninterruptible_device_2 = Uninterruptible(name = \"Washing Machine\", device_power_consumption = 0.5,\n",
    "                                            user_probabilities = uninterruptible_user_probabilities, device_standard_penalty = uninterruptible_device_standard_penalty, \n",
    "                                            device_on_duration = 1, device_override_penalty = uninterruptible_device_override_penalty)\n",
    "\n",
    "uninterruptible_device_3 = Uninterruptible(name = \"Clothes Dryer\", device_power_consumption = 2.4,\n",
    "                                            user_probabilities = uninterruptible_user_probabilities, device_standard_penalty = uninterruptible_device_standard_penalty, \n",
    "                                            device_on_duration = 0.5, device_override_penalty = uninterruptible_device_override_penalty)\n",
    "\n",
    "array_of_uninterruptible_devices = [uninterruptible_device_1, uninterruptible_device_2, uninterruptible_device_3]\n",
    "\n",
    "# Creating the environment\n",
    "neutral_train_env = EMSGymEnv(data_file = data_file, intermittent_devices = array_of_intermittent_devices,\n",
    "                uninterruptible_devices = array_of_uninterruptible_devices, episode_horizon = 7, time_step_duration = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conservative user in terms of energy - Prudent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment successfully initialized\n"
     ]
    }
   ],
   "source": [
    "# Assigning data file\n",
    "#data_file = \"prices_for_one_day_inference.csv\"\n",
    "data_file = \"nyiso_hourly_prices.csv\"\n",
    "\n",
    "# Creating the intermittent devices\n",
    "intermittent_user_probabilities = np.array([0.20, 0.80, 0.90, 0.20])\n",
    "\n",
    "intermittent_device_penalty = 100\n",
    "\n",
    "intermittent_device_1 = Intermittent(name = \"Small AC\", device_power_consumption = 1, \n",
    "                                        user_probabilities = intermittent_user_probabilities, device_standard_penalty = intermittent_device_penalty)\n",
    "\n",
    "intermittent_device_2 = Intermittent(name = \"Big AC\", device_power_consumption = 2.5, \n",
    "                                    user_probabilities = intermittent_user_probabilities, device_standard_penalty = intermittent_device_penalty)\n",
    "\n",
    "intermittent_device_3 = Intermittent(name = \"Ceiling Fan 1\", device_power_consumption = 0.07,\n",
    "                                    user_probabilities = intermittent_user_probabilities, device_standard_penalty = intermittent_device_penalty)\n",
    "\n",
    "intermittent_device_4 = Intermittent(name = \"Ceiling Fan 2\", device_power_consumption = 0.07,\n",
    "                                    user_probabilities = intermittent_user_probabilities, device_standard_penalty = intermittent_device_penalty)\n",
    "\n",
    "intermitent_device_5 = Intermittent(name = \"Boiler\", device_power_consumption = 3,\n",
    "                                    user_probabilities = intermittent_user_probabilities, device_standard_penalty = intermittent_device_penalty)\n",
    "\n",
    "intermittent_device_6 = Intermittent(name = \"Dehumidifier\", device_power_consumption = 0.07,\n",
    "                                    user_probabilities = intermittent_user_probabilities, device_standard_penalty = intermittent_device_penalty)\n",
    "\n",
    "array_of_intermittent_devices = [intermittent_device_1, intermittent_device_2, intermittent_device_3, intermittent_device_4, intermitent_device_5, intermittent_device_6]\n",
    "\n",
    "# Creating the uninterruptible devices\n",
    "\n",
    "uninterruptible_user_probabilities = np.array([0.20, 0.80, 0.90, 0.20])\n",
    "\n",
    "uninterruptible_device_standard_penalty = 100\n",
    "\n",
    "uninterruptible_device_override_penalty = 1000\n",
    "\n",
    "uninterruptible_device_1 = Uninterruptible(name = \"Dishwasher\", device_power_consumption = 1.3,\n",
    "                                            user_probabilities = uninterruptible_user_probabilities, device_standard_penalty = uninterruptible_device_standard_penalty, \n",
    "                                            device_on_duration = 2.5, device_override_penalty = uninterruptible_device_override_penalty)\n",
    "\n",
    "uninterruptible_device_2 = Uninterruptible(name = \"Washing Machine\", device_power_consumption = 0.5,\n",
    "                                            user_probabilities = uninterruptible_user_probabilities, device_standard_penalty = uninterruptible_device_standard_penalty, \n",
    "                                            device_on_duration = 1, device_override_penalty = uninterruptible_device_override_penalty)\n",
    "\n",
    "uninterruptible_device_3 = Uninterruptible(name = \"Clothes Dryer\", device_power_consumption = 2.4,\n",
    "                                            user_probabilities = uninterruptible_user_probabilities, device_standard_penalty = uninterruptible_device_standard_penalty, \n",
    "                                            device_on_duration = 0.5, device_override_penalty = uninterruptible_device_override_penalty)\n",
    "\n",
    "array_of_uninterruptible_devices = [uninterruptible_device_1, uninterruptible_device_2, uninterruptible_device_3]\n",
    "\n",
    "# Creating the environment\n",
    "prudent_train_env = EMSGymEnv(data_file = data_file, intermittent_devices = array_of_intermittent_devices,\n",
    "                uninterruptible_devices = array_of_uninterruptible_devices, episode_horizon = 7, time_step_duration = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training agents on the different user environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting CallBack Function to use that logs data to WandB service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WandBCallback(DefaultCallbacks):\n",
    "    def __init__(self, legacy_callbacks_dict: Dict[str, callable] = None):\n",
    "        self.episodes_counter = 0\n",
    "        self.episode_total_reward_counter = 0\n",
    "        self.intermittent_reward_counter = 0\n",
    "        self.uninterruptible_reward_counter = 0\n",
    "        self.cost_counter = 0\n",
    "\n",
    "        run = wandb.init(\n",
    "            # set the wandb project where this run will be logged\n",
    "            project=\"env_paper_experiments\",\n",
    "            name=\"receptive_train_env_t05_d07_A2C_100_100_1000_RUN\",  # receptive, resistant, neutral, prudent\n",
    "            tags=[\"7_days\", \"05_timestep\", \"6_pr_3_df\", \"A2C\", \"Receptive\"],\n",
    "            # # track hyperparameters and run metadata\n",
    "            # config={\n",
    "            # \"architecture\": \"Keras Simple Neural Network\",\n",
    "            # \"Environment\": \"SmartHomeGym_env_v2\",\n",
    "            # \"episodes\": 150,\n",
    "            # }\n",
    "        )\n",
    "\n",
    "        if legacy_callbacks_dict:\n",
    "            deprecation_warning(\n",
    "                \"callbacks dict interface\",\n",
    "                (\n",
    "                    \"a class extending rllib.algorithms.callbacks.DefaultCallbacks; see\"\n",
    "                    \" `rllib/examples/custom_metrics_and_callbacks.py` for an example.\"\n",
    "                ),\n",
    "                error=True,\n",
    "            )\n",
    "\n",
    "        # print(\"Episode Initialized\")\n",
    "\n",
    "    def on_episode_created(\n",
    "        self,\n",
    "        *,\n",
    "        worker: \"RolloutWorker\",\n",
    "        base_env: BaseEnv,\n",
    "        policies: Dict[PolicyID, Policy],\n",
    "        env_index: int,\n",
    "        episode: Union[Episode, EpisodeV2],\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "        # self.episode_id = env_index\n",
    "        self.episode_id = random.randint(1, 10000)\n",
    "\n",
    "    def on_episode_end(\n",
    "        self,\n",
    "        *,\n",
    "        worker: \"RolloutWorker\",\n",
    "        base_env: BaseEnv,\n",
    "        policies: Dict[PolicyID, Policy],\n",
    "        episode: Union[Episode, EpisodeV2, Exception],\n",
    "        env_index: Optional[int] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.episodes_counter += 1\n",
    "        self.intermittent_reward_counter += episode._last_infos[\"agent0\"][\n",
    "            \"intermittent_device_reward_total\"\n",
    "        ]\n",
    "        self.uninterruptible_reward_counter += episode._last_infos[\"agent0\"][\n",
    "            \"uninterruptible_device_reward_total\"\n",
    "        ]\n",
    "        self.episode_total_reward_counter += episode.total_reward\n",
    "        self.plot_kwh = episode._last_infos[\"agent0\"][\"kwh_device_history\"]\n",
    "        self.plot_price = episode._last_infos[\"agent0\"][\"price_history\"]\n",
    "        self.plot_time = episode._last_infos[\"agent0\"][\"time\"]\n",
    "        self.cost_counter += np.sum(\n",
    "            np.array(self.plot_kwh) * np.array(self.plot_price) * 0.5\n",
    "        )\n",
    "\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"episode_reward_total\": episode.total_reward,\n",
    "                \"episode_reward_mean\": self.episode_total_reward_counter\n",
    "                / self.episodes_counter,\n",
    "                \"intermittent_device_reward_total\": episode._last_infos[\"agent0\"][\n",
    "                    \"intermittent_device_reward_total\"\n",
    "                ],\n",
    "                \"uninterruptible_device_reward_total\": episode._last_infos[\"agent0\"][\n",
    "                    \"uninterruptible_device_reward_total\"\n",
    "                ],\n",
    "                \"intermittent_device_reward_mean\": self.intermittent_reward_counter\n",
    "                / self.episodes_counter,\n",
    "                \"uninterruptible_device_reward_mean\": self.uninterruptible_reward_counter\n",
    "                / self.episodes_counter,\n",
    "                \"cost\": self.cost_counter / self.episodes_counter,\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the different agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-17 14:33:51,945\tINFO tune.py:922 -- Initializing Ray automatically.For cluster usage or custom Ray initialization, call `ray.init(...)` before `tune.run`.\n",
      "2024-07-17 14:33:54,822\tINFO worker.py:1553 -- Started a local Ray instance.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-07-17 14:47:44</td></tr>\n",
       "<tr><td>Running for: </td><td>00:13:46.96        </td></tr>\n",
       "<tr><td>Memory:      </td><td>14.8/15.2 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/1 GPUs, 0.0/1.99 GiB heap, 0.0/0.99 GiB objects\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "<div class=\"messages\">\n",
       "  <h3>Messages</h3>\n",
       "  : ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
       "  \n",
       "  \n",
       "</div>\n",
       "<style>\n",
       ".messages {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  padding-left: 1em;\n",
       "  overflow-y: auto;\n",
       "}\n",
       ".messages h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n",
       "\n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status    </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>A2C_SmartHomeGymEnv_7497d_00000</td><td>TERMINATED</td><td>127.0.0.1:8256</td><td style=\"text-align: right;\">   200</td><td style=\"text-align: right;\">         803.308</td><td style=\"text-align: right;\">67200</td><td style=\"text-align: right;\"> -227172</td><td style=\"text-align: right;\">             -207792</td><td style=\"text-align: right;\">             -257264</td><td style=\"text-align: right;\">               336</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-17 14:33:57,708\tINFO algorithm_config.py:2888 -- Executing eagerly (framework='tf2'), with eager_tracing=tf2. For production workloads, make sure to set eager_tracing=True  in order to match the speed of tf-static-graph (framework='tf'). For debugging purposes, `eager_tracing=False` is the best choice.\n",
      "2024-07-17 14:33:57,709\tINFO algorithm_config.py:2888 -- Executing eagerly (framework='tf2'), with eager_tracing=tf2. For production workloads, make sure to set eager_tracing=True  in order to match the speed of tf-static-graph (framework='tf'). For debugging purposes, `eager_tracing=False` is the best choice.\n",
      "\u001b[2m\u001b[36m(A2C pid=8256)\u001b[0m 2024-07-17 14:34:05,637\tWARNING algorithm_config.py:596 -- Cannot create A2CConfig from given `config_dict`! Property __stdout_file__ not supported.\n",
      "\u001b[2m\u001b[36m(A2C pid=8256)\u001b[0m 2024-07-17 14:34:05,638\tINFO algorithm_config.py:2888 -- Executing eagerly (framework='tf2'), with eager_tracing=tf2. For production workloads, make sure to set eager_tracing=True  in order to match the speed of tf-static-graph (framework='tf'). For debugging purposes, `eager_tracing=False` is the best choice.\n",
      "\u001b[2m\u001b[36m(A2C pid=8256)\u001b[0m 2024-07-17 14:34:06,139\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(A2C pid=8256)\u001b[0m 2024-07-17 14:34:06,148\tWARNING env.py:156 -- Your env doesn't have a .spec.max_episode_steps attribute. Your horizon will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(A2C pid=8256)\u001b[0m 2024-07-17 14:34:06,148\tWARNING env.py:166 -- Your env reset() method appears to take 'seed' or 'return_info' arguments. Note that these are not yet supported in RLlib. Seeding will take place using 'env.seed()' and the info dict will not be returned from reset.\n",
      "\u001b[2m\u001b[36m(A2C pid=8256)\u001b[0m 2024-07-17 14:34:06,336\tWARNING util.py:67 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th style=\"text-align: right;\">  agent_timesteps_total</th><th>connector_metrics                                                                                                                                           </th><th>counters                                                                                                                            </th><th>custom_metrics  </th><th>date               </th><th>done  </th><th style=\"text-align: right;\">  episode_len_mean</th><th>episode_media  </th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_mean</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episodes_this_iter</th><th style=\"text-align: right;\">  episodes_total</th><th>experiment_id                   </th><th>hostname       </th><th>info                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              </th><th style=\"text-align: right;\">  iterations_since_restore</th><th>node_ip  </th><th style=\"text-align: right;\">  num_agent_steps_sampled</th><th style=\"text-align: right;\">  num_agent_steps_trained</th><th style=\"text-align: right;\">  num_env_steps_sampled</th><th style=\"text-align: right;\">  num_env_steps_sampled_this_iter</th><th style=\"text-align: right;\">  num_env_steps_trained</th><th style=\"text-align: right;\">  num_env_steps_trained_this_iter</th><th style=\"text-align: right;\">  num_faulty_episodes</th><th style=\"text-align: right;\">  num_healthy_workers</th><th style=\"text-align: right;\">  num_in_flight_async_reqs</th><th style=\"text-align: right;\">  num_remote_worker_restarts</th><th style=\"text-align: right;\">  num_steps_trained_this_iter</th><th>perf                                                                          </th><th style=\"text-align: right;\">  pid</th><th>policy_reward_max  </th><th>policy_reward_mean  </th><th>policy_reward_min  </th><th>sampler_perf                                                                                                                                                                                                   </th><th>sampler_results                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     </th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th>timers                                                                                                                         </th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  timesteps_since_restore</th><th style=\"text-align: right;\">  timesteps_total</th><th style=\"text-align: right;\">  training_iteration</th><th>trial_id   </th><th style=\"text-align: right;\">  warmup_time</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>A2C_SmartHomeGymEnv_7497d_00000</td><td style=\"text-align: right;\">                  67200</td><td>{&#x27;ObsPreprocessorConnector_ms&#x27;: 0.0485682487487793, &#x27;StateBufferConnector_ms&#x27;: 0.010799169540405273, &#x27;ViewRequirementAgentConnector_ms&#x27;: 0.1335616111755371}</td><td>{&#x27;num_env_steps_sampled&#x27;: 67200, &#x27;num_env_steps_trained&#x27;: 67200, &#x27;num_agent_steps_sampled&#x27;: 67200, &#x27;num_agent_steps_trained&#x27;: 67200}</td><td>{}              </td><td>2024-07-17_14-47-44</td><td>True  </td><td style=\"text-align: right;\">               336</td><td>{}             </td><td style=\"text-align: right;\">             -207792</td><td style=\"text-align: right;\">              -227172</td><td style=\"text-align: right;\">             -257264</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">             200</td><td>c6cf490cd7c94949837005e6910691e5</td><td>LAPTOP-20VM6M02</td><td>{&#x27;learner&#x27;: {&#x27;default_policy&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_lr&#x27;: 0.0010000000474974513, &#x27;entropy_coeff&#x27;: 0.009999999776482582, &#x27;policy_loss&#x27;: -inf, &#x27;policy_entropy&#x27;: 5209.105, &#x27;var_gnorm&#x27;: 22.72548, &#x27;vf_loss&#x27;: 365806700000.0}, &#x27;grad_gnorm&#x27;: 0.0, &#x27;vf_explained_var&#x27;: -0.00027763844, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 336, &#x27;num_grad_updates_lifetime&#x27;: 200, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 0.0}}, &#x27;num_env_steps_sampled&#x27;: 67200, &#x27;num_env_steps_trained&#x27;: 67200, &#x27;num_agent_steps_sampled&#x27;: 67200, &#x27;num_agent_steps_trained&#x27;: 67200}</td><td style=\"text-align: right;\">                       200</td><td>127.0.0.1</td><td style=\"text-align: right;\">                    67200</td><td style=\"text-align: right;\">                    67200</td><td style=\"text-align: right;\">                  67200</td><td style=\"text-align: right;\">                              336</td><td style=\"text-align: right;\">                  67200</td><td style=\"text-align: right;\">                              336</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                         0</td><td style=\"text-align: right;\">                           0</td><td style=\"text-align: right;\">                          336</td><td>{&#x27;cpu_util_percent&#x27;: 4.414285714285714, &#x27;ram_util_percent&#x27;: 97.45714285714284}</td><td style=\"text-align: right;\"> 8256</td><td>{}                 </td><td>{}                  </td><td>{}                 </td><td>{&#x27;mean_raw_obs_processing_ms&#x27;: 0.3854113005684867, &#x27;mean_inference_ms&#x27;: 10.257892434342613, &#x27;mean_action_processing_ms&#x27;: 0.3227326379466809, &#x27;mean_env_wait_ms&#x27;: 0.5600495017778483, &#x27;mean_env_render_ms&#x27;: 0.0}</td><td>{&#x27;episode_reward_max&#x27;: -207792.45152509795, &#x27;episode_reward_min&#x27;: -257264.4572687586, &#x27;episode_reward_mean&#x27;: -227171.60427793875, &#x27;episode_len_mean&#x27;: 336.0, &#x27;episode_media&#x27;: {}, &#x27;episodes_this_iter&#x27;: 1, &#x27;policy_reward_min&#x27;: {}, &#x27;policy_reward_max&#x27;: {}, &#x27;policy_reward_mean&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;hist_stats&#x27;: {&#x27;episode_reward&#x27;: [-220756.73268277873, -226931.58847334838, -239525.18434532644, -231641.93814757915, -232759.38813185523, -233961.7590762106, -229504.1306351432, -234618.0821383301, -219220.2879527188, -231255.68463683221, -223211.02762103683, -220798.73202328666, -213548.76592040248, -214684.68837276107, -231426.4226523542, -257264.4572687586, -212252.63304893495, -225010.86181185415, -233597.92927511584, -219230.0449535457, -239282.20056955595, -225694.5431875746, -228947.31185170158, -223192.08987688564, -228265.23477742236, -237106.92855237136, -233876.00302591655, -221684.06690231612, -223831.65004670314, -214502.94099020254, -221470.3976319886, -222253.00449732415, -230522.4463605329, -217621.96899376824, -223190.1663978473, -245949.6201999498, -226459.38252481227, -231791.3930311534, -235659.0857714662, -225948.9102788041, -207792.45152509795, -234179.283592744, -222264.4430058272, -230569.49449584816, -242688.1281920162, -232724.1991577191, -229875.47573041404, -223232.57107325076, -218342.38443063214, -213354.70401705525, -224275.51637198887, -230824.93218960383, -224454.26114317574, -224088.93997593448, -228328.98158074156, -239412.27160162205, -215070.98708176357, -234919.92345713527, -229001.00508607508, -234724.79450701355, -239761.0077800419, -225796.4796504628, -221622.2528933678, -221634.31941002308, -212506.4522605994, -224696.13057999167, -217488.35472274353, -227593.60122704325, -241046.2756751399, -225303.41145902366, -227749.88436590217, -229901.97733623188, -219455.48142572847, -231387.78706173133, -215623.3186440723, -219432.6919113, -234334.1628632744, -231129.04521980806, -215662.2431697751, -241176.63934030558, -238012.8130097922, -241909.19876280037, -224365.66588358447, -215293.97715332056, -230379.16771162435, -227268.33985039633, -216982.98051740593, -213751.5457383768, -233913.01104624363, -231252.6429290289, -226490.0059461095, -214586.98272462064, -238227.93399559805, -231406.79509348536, -228488.18997045653, -238780.20801724863, -226531.93198251008, -230410.36919452343, -227594.4039809173, -209638.314437137], &#x27;episode_lengths&#x27;: [336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336]}, &#x27;sampler_perf&#x27;: {&#x27;mean_raw_obs_processing_ms&#x27;: 0.3854113005684867, &#x27;mean_inference_ms&#x27;: 10.257892434342613, &#x27;mean_action_processing_ms&#x27;: 0.3227326379466809, &#x27;mean_env_wait_ms&#x27;: 0.5600495017778483, &#x27;mean_env_render_ms&#x27;: 0.0}, &#x27;num_faulty_episodes&#x27;: 0, &#x27;connector_metrics&#x27;: {&#x27;ObsPreprocessorConnector_ms&#x27;: 0.0485682487487793, &#x27;StateBufferConnector_ms&#x27;: 0.010799169540405273, &#x27;ViewRequirementAgentConnector_ms&#x27;: 0.1335616111755371}}</td><td style=\"text-align: right;\">             803.308</td><td style=\"text-align: right;\">           4.64905</td><td style=\"text-align: right;\">       803.308</td><td>{&#x27;training_iteration_time_ms&#x27;: 4815.217, &#x27;learn_time_ms&#x27;: 54.149, &#x27;learn_throughput&#x27;: 6205.144, &#x27;synch_weights_time_ms&#x27;: 0.452}</td><td style=\"text-align: right;\"> 1721216864</td><td style=\"text-align: right;\">                        0</td><td style=\"text-align: right;\">            67200</td><td style=\"text-align: right;\">                 200</td><td>7497d_00000</td><td style=\"text-align: right;\">     0.214103</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "2024-07-17 14:47:44,832\tINFO tune.py:798 -- Total run time: 827.13 seconds (826.95 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "seed_value = 42\n",
    "\n",
    "receptive_train_env.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "random.seed(seed_value)\n",
    "\n",
    "register_env(\n",
    "    \"SmartHomeGymEnv\", lambda config: prudent_train_env\n",
    ")  # receptive, resistant, neutral, prudent\n",
    "\n",
    "config = AlgorithmConfig()\n",
    "\n",
    "# config = config.training(lr=0.001,train_batch_size=300, model={\"fcnet_hiddens\": [64,128,128], \"fcnet_activation\" : \"relu\"})\n",
    "# config = config.training(lr=0.01,train_batch_size=336)\n",
    "config = config.training(\n",
    "    train_batch_size=300,\n",
    "    model={\n",
    "        \"fcnet_hiddens\": [256, 256],\n",
    "        \"fcnet_activation\": \"swish\",\n",
    "        \"post_fcnet_hiddens\": [],\n",
    "        \"post_fcnet_activation\": \"swish\",\n",
    "    },\n",
    ")\n",
    "config = config.environment(env=\"SmartHomeGymEnv\")\n",
    "config = config.resources(num_gpus=1)\n",
    "config = config.rollouts(\n",
    "    num_rollout_workers=0,\n",
    "    batch_mode=\"complete_episodes\",\n",
    "    rollout_fragment_length=300,\n",
    "    num_envs_per_worker=1,\n",
    ")\n",
    "#config = config.callbacks(WandBCallback)\n",
    "config = config.framework(framework=\"tf2\")\n",
    "config = config.resources(num_gpus=0, num_trainer_workers=1)\n",
    "\n",
    "config = config.debugging(seed=42)\n",
    "\n",
    "config.offline_data(postprocess_inputs=True)\n",
    "\n",
    "run_config = RunConfig()\n",
    "\n",
    "run_config.name = \"A2C/penalties_100_100_1000\"\n",
    "run_config.stop = {\"episodes_total\": 200}\n",
    "run_config.local_dir = (\n",
    "    \"prudent_environment_logs\"  # receptive, resistant, neutral, prudent\n",
    ")\n",
    "\n",
    "checkpoint_config = CheckpointConfig()\n",
    "\n",
    "checkpoint_config.num_to_keep = 2\n",
    "checkpoint_config.checkpoint_score_order = \"max\"\n",
    "checkpoint_config.checkpoint_score_attribute = \"episode_reward_mean\"\n",
    "checkpoint_config.checkpoint_frequency = 1\n",
    "\n",
    "run_config.checkpoint_config = checkpoint_config\n",
    "\n",
    "best_result = tune.Tuner(\n",
    "    trainable=\"A2C\", param_space=config.to_dict(), run_config=run_config\n",
    ").fit()\n",
    "\n",
    "wandb.finish()\n",
    "ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
